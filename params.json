{
  "name": "CS109a Chicago Food Inspections",
  "tagline": "Andrew Brennan and Joe Reilly",
  "body": "# Introduction\r\n\r\nIn one year in Chicago, more than 15,000 food vending establishments are inspected by 30 - 35 Department of Public Health (DPH) inspectors. Many of these inspections uncover no serious issues, but roughly 15% of inspections detect establishments failing to adhere to DPH guidelines. The longer a restaurant or other location operates with a serious violation, the greater the risk of food poisoning or other illness for Chicago’s citizens. This project focuses on predicting which establishments are likely to have serious violations and aims to prioritize visiting these locations soonest. \r\n\r\nFar from being a hypothetical exercise, the City of Chicago partnered with Allstate Insurance in 2014 to tackle this problem. Chicago is a pioneer of providing open-source data and was recently awarded $1M from the Bloomberg Philanthropies Mayor’s Challenge in which they proposed to aggregate and analyze data to make better and faster decisions.  This led to the creation of the SmartData Platform, a place to access well-structured and up-to-date data from many sources including business licensing, crime statistics, 311 complaints about sanitation, and prior food inspection outcomes. The open data portal that exists to improve transparency for city government, allow developers to make apps to aid residents, and to increase commercial activity for small and large businesses. The portal aided the city’s dissemination of a large volume of data and proved to be an effective tool for allowing collaborative research.\r\n\r\nThe City of Chicago has released its data and code for this project with the hopes that other cities can implement similar procedures or help them improve their own.  However, most other cities would struggle to aggregate the data from many sources since they do not have a program similar to the SmartData Platform.  Our goal in this project was to study their process, develop our own model, and see what performance gains we could achieve.\r\n\r\n# Data  \r\n\r\nOur primary source of data for the project was a repository of roughly 340,000 restaurant inspections in the City of Chicago from 2010 to the present. This repository is freely hosted on the city’s Data Portal and includes basic information such as name, location, time and date of inspection, type of facility, and license number. Location  was represented as both zip code and as latitude/longitude coordinates. Facilities are classified into different risk tiers with Risk 1 (High) establishments presenting the highest risk to the population due to handling of raw ingredients (such as a restaurant) to Risk 3 (Low) facilities that typically reheat premade food (such as some daycares). We did not eliminate facilities by risk, but we chose to narrow our data to restaurants, grocery stores, and bakeries due to the assigned topic of the project on retail food establishments.  \r\n\r\nWe calculated days since last inspection and noticed that Risk 1 establishments do get inspected more frequently than lower risk restaurants. We surmised that the length of time between inspections could be an important predictor of inspection outcome.\r\n\r\n![](http://i.imgur.com/aQJvYLZ.png)\r\n\r\nMultiple types of inspection are possible, but many inspections such as those for establishing licensure must occur at certain times and are not able to be prioritized by our model. We narrowed our focus to inspections that occurred due to regular canvassing, as a result of a complaint, or due to suspected food poisoning. Inspection outcomes fall into three categories: pass, pass with conditions, and fail. The Chicago/Allstate team collapsed this into pass/fail, but we kept the outcome multiclass to see if this added nuance would aid the predictive power of our model. We also removed all inspections where no outcome was listed due to the establishment going out of business or being under renovations. After this filtering, we had 61,727 records remaining.  \r\n\r\n![](http://i.imgur.com/V4dMuDh.jpg)\r\n\r\nHistorical weather data were available via the NOAA National Centers for Environmental Information. These data were recorded at Chicago O’Hare international airport and we limited ourselves to using the daily high, low, and average temperature, the amount of precipitation, the amount of snowfall, and the average wind speed. From the previous analysis done by the Chicago/Allstate team, we believed the temperature data may predict inspection outcomes to some degree. In lieu of using the daily high, we calculated the three-day rolling average high temperature.  \r\n\r\nWe downloaded all the 311 sanitation complaints in the city, of which there were 112,000, or about 45 per day. These complaints may reflect areas of the city where sanitation standards are reduced and therefore may predict where restaurants will fail their inspections. To utilize these data in our task, we tallied the number of complaints within the last week and within a 0.5 km radius for each inspection.  \r\n\r\nFinally, we scraped rating and price data from Yelp for our retail food establishments due to a suggestion from the teaching team during our poster session. Due to limits on the number of calls allowed to the Yelp API per day and the short period of time between the poster session and the website due date, we were able to search for roughly 16,000 establishments. After removing missing data and narrowing our focus on certain types of establishment and inspections, we obtained a subset of 8,337 inspection records with price and review data. Analysis of this subset will allow us to determine if it would be worthwhile to continue scraping this data for more establishments.  \r\n\r\n# Model Building\r\n\r\nWe first defined our performance metric against which all our models would be evaluated.  We aimed to maximize the likelihood of the observed data given the model.  For computational reasons, we are technically minimizing the negative log likelihood as a loss function. The log likelihood function adds a positive number with a magnitude of roughly 1 for each additional observation. This is much less likely to hit machine precision issues than multiplying many numbers smaller than 1 as in the likelihood function, which gets quite small quite fast.  \r\n\r\nThe model assigns a probability to each of the three classes of inspection outcome, and then the probability of the observed outcome (according to the model) is then just the class probability of the observed outcome. The joint probability of all the observed outcomes, assuming they are independent, is the product of the individual outcome probabilities.  We are scoring the models based on this joint probability of obtaining the observed data from that model.  This metric has two advantages:  it is the same metric that logistic regression models maximize and it naturally extends to the multiclass classification we are doing here. All visuals in our model selection process plot this likelihood using the characteristic probability, which represents an average probability of observing the data across all the observations.  Specifically, if model M has characteristic probability P, then the model that always predicts the observed outcome with probability P will have the same likelihood.  \r\n\r\nWe aim to maximize the likelihood on out-of-sample data.  However, we are dealing with time-series data, so simple K-fold cross-validation would be invalid, using information from the future to inform the predictions, whereas our use-case is to train the model on existing data to predict values in the future.  We therefore used the sklearn package TimeSeriesCV to do the cross-validation.  For n_splits, this makes n_splits testing sets, and for each one the model is trained on all past data but no future data.  \r\n\r\nOur first baseline model simply uses the inspection outcome frequencies from our sample to make predictions.  Regardless of the inspection, it predicts the same class probabilities.  This gave a characteristic probability of 0.40 which we sought to improve upon.  We added the day of the week, month of the year, and zip code to our model, but cross-validation revealed that this model overfits our data.\r\n\r\n![](http://i.imgur.com/PgYnAwK.png)\r\n\r\n(Technical note: The above plot uses all inspection types, not just canvasses, complaints, and suspected food poisoning.  That is why the characteristic probabilities are elevated compared to all future plots.)\r\n\r\nWe then fit a series of logistic regression models where we added the month, year, and zip code. None of these factors helped the model.  Year, in particular, hurt the model particularly badly.  This happened because, using the time-series based cross-validation, there are never many training samples of the same year in the testing data. No further models include year as a predictor.\r\n\r\n![](http://i.imgur.com/DbF2geH.png)\r\n\r\nOur next series of models took this information and incorporated risk, facility type, and inspection type. The type of inspection being carried out holds the most predictive power of these variables with high-risk inspections more often resulting in critical failures. Including risk rating and facility type also serve to improve our model. The model that included these three additional features performed better, both in-sample and out-of-sample (cross-validation likelihood).  \r\n\r\n![](http://i.imgur.com/lO1CN9G.png)\r\n\r\nAdding all these factors together led to a better model, indicating that they have independent information and we are not yet in the domain of overfitting.\r\n\r\n![](http://i.imgur.com/wjR9dns.png)\r\n\r\nWe next tried to improve this model by adding recent nearby sanitation complaints and weather data.  For the sanitation complaints, we took complaints within 0.5km (much smaller than the ~3km radius we used before) and within the last week.  For weather, we looked at both the day’s high and the mean high from the last week.  Both of these additions helped the model perform better, although both were weak.  \r\n\r\n![](http://i.imgur.com/Ve7DBRs.png)\r\n\r\nThe inclusion of review and price data scraped from Yelp for a subset of our inspection data did not seem to improve the predictive power of our model. It is possible that this could be more significant over a larger dataset or with more complete data, but for now we will not include it in our final model. Further work will need to be done to test the hypothesis suggested during our poster session that more expensive establishments have more to lose from a failure and would thus take additional precautions to fail at a lower rate. The converse could also be true, where a low price establishment operates with razor-thin margins and a closure could permanently shutter the location therefore driving more scrupulous preventative measures.\r\n\r\n![](http://i.imgur.com/rjAfaR2.jpg)\r\n\r\nTo delve into inspection data further, we added time since last inspection, the result of the previous inspection, and the mean of all previous results. This led to our final logistic regression model that included location by zip code, inspection type, facility type, and risk rating alongside our newly added predictors.  \r\n\r\n![](http://i.imgur.com/7MAEOLX.png)\r\n\r\n## Spatial Analysis\r\n\r\nIn the models described above, we coded location using the zip codes, after eliminating the 40% or so of zip codes that did not have 1000 inspections (they were coded together as \"other\").  This has the benefit of roughly scaling to population size, as city-center zip codes cover much less area, as well as convenience as it was already coded.  This factor picks up the fact that the probability of passing an inspection depends on where the restaurant is located, i.e. there are \"good\" parts of town and \"bad\" parts of town with respect to food inspections.\r\n\r\nWe could alternatively use nearby sanitation complaints as a proxy for \"good\" and \"bad\" parts of town, under the hypothesis that more nearby recent sanitation complaints indicate a part of town that is more likely to fail food inspections.  This has the advantage of continuously varying throughout the city, but the disadvantage of being only a proxy for passing inspections rather than a direct measure of past performance.  Below, we see that as the number of nearby recent complaints increases, the probability of passing an inspection falls and the probability of critical failures rises.\r\n\r\n![](http://i.imgur.com/2spzQA8.png)\r\n\r\nWhen we tried a model of just zip codes or just sanitation complaints, the sanitation complaints actually outperformed the zip codes.  When adding both to the model, it did not perform any better, indicating that the two measures capture very similar information.  This is likely why we did not see improvements to the model above using sanitation complaints: zip codes were already capturing the bulk of the spatial information.\r\n\r\n![](http://i.imgur.com/mpO73DD.png)\r\n\r\nWe next wondered whether we could capture the benefits of both approaches: a measure that continuously varies through the city but that directly measures past performance on inspections.  We tried using Kernel Density Estimation to map the city.  For each passed inspection, we drew a Gaussian distribution centered at that location with variance 1.5km.  We did this for each passed inspection in a period prior to the testing period, making a map of small bumps.  We made analogous maps for conditional fails and critical fails.  Then, for a future inspection, we compared the height of the passing map to the conditional fail to the critical fail, giving a measure of how likely it is that a restaurant in that area fails.  \r\n\r\nA model using the KDE approach to encoding the spatial information outperforms both the sanitation and zip approaches.  Unfortunately, we were not able to get this approach working early enough to incorporate it into a logistic regression model and add it to the other features.  However, this is a very promising direction for improving our model.\r\n\r\n![](http://i.imgur.com/ubrgLdP.png)\r\n\r\n## Alternative Models\r\n\r\nAfter being satisfied that we maximized the performance of our logistic regression model, we attempted to fit several new models using different classifiers. We first attempted to use linear and quadratic discriminant analysis, but these did not perform nearly as well as logistic regression. This is likely due to their difficulty dealing with large amounts of categorical data. We next attempted a k-nearest neighbors approach to our classification problem. As the number of nearest neighbors considered increased past 75, k-NN performed roughly as well as our logistic regression model. Two issues arose, however: overfitting of our data was detected and the model was very computationally expensive to run.\r\n\r\n![](http://i.imgur.com/mALmPQP.png)\r\n\r\nWe next turned to tree-based classifiers. The Random Forest classifier slightly outperformed our logistic regression model on characteristic probability after careful tuning with cross-validation. Further discussion in the “Evaluation” section will compare the performance of these two models. The AdaBoost classifier was also utilized but did not perform as well with our predictors as the Random Forest.\r\n\r\n![](http://i.imgur.com/tVHwi1G.png)\r\n\r\nA visualization of the performance of our final models is provided below:\r\n\r\n![](http://i.imgur.com/YBIk3X9.png)\r\n\r\n# Evaluation\r\n\r\nTo quantify the performance of our models in a more practical sense, we trained our models on our data through 2015 then had them predict inspection outcomes for the first nine months of 2016. We sorted their monthly results by risk of failure and explored how many failures were caught in the first half of the month. This metric aligns with our project goal of detecting failures sooner to minimize risk to the public. As seen in the figure below, our logistic regression and Random Forest models both did significantly better than Chicago's current method, detecting on average 25% more failures in the first half of the month. The Random Forest model outperforms logistic regression in 5 out of 9 months of our test set.\r\n\r\n<center>![](http://i.imgur.com/KLATIQD.png)</center>\r\n\r\nThe most important features of our Random Forest classifier are visualized below. Indicator variables for if the inspection was a result of either type of complaint ranked highly, along with days elapsed since last inspection, the result of that last inspection, and the mean result of all inspections on record for that establishment.\r\n\r\n![](http://i.imgur.com/nWdjmpG.png)\r\n\r\n# Future Directions\r\n\r\nAs we mentioned above, a very promising direction to improve our model is to incorporate the local history of passing inspections based the Kernel Density Estimates.  These outperformed both the sanitation and zip-based encoding of spatial information and would likely be a very helpful addition to the model.\r\n\r\nTo improve our model at this point, we could explore more nuanced information about establishments such as price and rating that were only partially available to us at the time of this analysis. We also found evidence that sanitation data may be helpful after restricting ourselves to inspections as a result of canvass or complaints, so we could see if this is beneficial on top of the KDE location information.  \r\n\r\nWe could explore interactions between the main effects we present here since, for example, temperature may have a different magnitude of effect at different locations. We could improve our model if we had access to data regarding what inspector carried out which inspection, but this is not made public by the City of Chicago. Inspection outcomes nested within inspectors would give us an interesting multilevel modeling problem to explore. We could also attempt to utilize key phrases in user reviews of establishments such as \"dirty fork\" or \"smelled funny\" to detect potential violations well before they might be caught by the baseline model. \r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}